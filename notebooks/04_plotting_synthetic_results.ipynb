{
 "cells": [
  {
   "cell_type": "code",
   "id": "81914c62ec010e48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T19:24:14.454235Z",
     "start_time": "2025-09-10T19:24:14.363873Z"
    }
   },
   "source": [
    "import pandas as pd, ast, textwrap\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "import matplotlib as mpl\n",
    "# Ensure we get DejaVu Sans (ships with matplotlib & covers most unicode):\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "# You can also explicitly set:\n",
    "mpl.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Methoden ausschließen - HIER ÄNDERN!\n",
    "# -------------------------------\n",
    "# Definiere hier die Methoden, die du aus den Plots ausschließen möchtest\n",
    "excluded_methods = {}# { 'nimo_baseline', 'NeuralNet2', 'lassonet', 'nimo_variant'}  # Beispiel: randomForest und nimoNew ausschließen\n",
    "# excluded_methods = set()  # Leeres Set = alle Methoden anzeigen\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Daten einlesen & vorbereiten\n",
    "# -------------------------------\n",
    "#df = pd.read_csv('../results/synthetic/all_model_results_synthetic_20Iterations_AUGUST.csv')\n",
    "df = pd.read_csv('../results/synthetic/clean_experiment_results.csv')\n",
    "df = df[df['error'].isna()] if 'error' in df else df\n",
    "df['selected_features'] = df['selected_features'].fillna('[]')\\\n",
    "    .apply(lambda s: ast.literal_eval(s) if isinstance(s, str) else [])\n",
    "df['n_selected'] = df['selected_features'].apply(len)\n",
    "for c in ['best_f1','n_selected','n_true_features']:\n",
    "    if c in df: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "short_map = df.groupby('scenario')['scenario_short'].first().to_dict()\n",
    "long_map  = df.groupby('scenario')['scenario_long'].first().to_dict()\n",
    "scenarios = sorted(df['scenario'].unique())\n",
    "\n",
    "# Methoden filtern - AUSSCHLUSS HIER ANWENDEN\n",
    "all_methods = sorted(df['model_name'].unique())\n",
    "methods = [m for m in all_methods if m not in excluded_methods]\n",
    "\n",
    "print(f\"Alle verfügbaren Methoden: {all_methods}\")\n",
    "print(f\"Ausgeschlossene Methoden: {excluded_methods}\")\n",
    "print(f\"Verwendete Methoden: {methods}\")\n",
    "\n",
    "palette   = dict(zip(methods, sns.color_palette(\"tab10\", len(methods))))\n",
    "linestyles = ['-','--','-.',':',(0,(5,1))]\n",
    "markers    = ['o','s','D','^','v']\n",
    "total_features = df['n_features_total'].iloc[0]\n",
    "\n",
    "for scen in scenarios:\n",
    "    grp = df[df['scenario']==scen]\n",
    "    desc_s = short_map[scen]\n",
    "    desc_l = long_map.get(scen,'')\n",
    "\n",
    "    # pick each model's best‐f1 row\n",
    "    best = grp.sort_values('best_f1', ascending=False).drop_duplicates('model_name')\n",
    "    cm = []\n",
    "    for m in methods:\n",
    "        row = best[best['model_name']==m].iloc[0]\n",
    "        S = {int(f.split('_')[-1]) for f in row['selected_features']}\n",
    "        T = set(ast.literal_eval(row['true_support'])) if isinstance(row['true_support'], str) else set(row['true_support'])\n",
    "        TP=len(S&T); FP=len(S-T); FN=len(T-S); TN=total_features-len(S|T)\n",
    "        cm.append((m,TP,FP,FN,TN))\n",
    "    cm_df = pd.DataFrame(cm, columns=['model','TP','FP','FN','TN']).set_index('model')\n",
    "\n",
    "    # --- layout ---\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs  = fig.add_gridspec(4,2,\n",
    "        height_ratios=[0.4, 3, 3, 1.5],\n",
    "        width_ratios =[3, 2],\n",
    "        hspace=0.8, wspace=0.3\n",
    "    )\n",
    "\n",
    "    fig.subplots_adjust(bottom=0.05)\n",
    "\n",
    "    # title & subtitle row (spans both cols)\n",
    "    axT = fig.add_subplot(gs[0,:])\n",
    "    axT.axis('off')\n",
    "    axT.text(0.5,1, scen + \": \"+ desc_s, fontsize=20, fontweight='bold', ha='center')\n",
    "    axT.text(0.5,0, \"\\n\".join(textwrap.wrap(desc_l, 60)),\n",
    "             fontsize=12, style='italic', ha='center')\n",
    "\n",
    "    # F1 plot\n",
    "    ax1 = fig.add_subplot(gs[1,0])\n",
    "    for i,m in enumerate(methods):\n",
    "        sub=grp[grp['model_name']==m]\n",
    "        ax1.plot(sub['iteration'],sub['best_f1'],\n",
    "                 color=palette[m],\n",
    "                 linestyle=linestyles[i%len(linestyles)],\n",
    "                 marker=markers[i%len(markers)],\n",
    "                 label=m)\n",
    "    ax1.set_title(\"Best F1 over Iterations\"); ax1.set_xlabel(\"Iter\"); ax1.set_ylabel(\"F1\")\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax1_acc = fig.add_subplot(gs[2,0])\n",
    "    for i,m in enumerate(methods):\n",
    "        sub=grp[grp['model_name']==m]\n",
    "        # Only plot accuracy if the column exists and has non-null values\n",
    "        if 'accuracy_test' in sub.columns and not sub['accuracy_test'].isna().all():\n",
    "            ax1_acc.plot(sub['iteration'],sub['accuracy_test'],\n",
    "                     color=palette[m],\n",
    "                     linestyle=linestyles[i%len(linestyles)],\n",
    "                     marker=markers[i%len(markers)],\n",
    "                     label=m)\n",
    "    ax1_acc.set_title(\"Test Accuracy over Iterations\"); ax1_acc.set_xlabel(\"Iter\"); ax1_acc.set_ylabel(\"Accuracy\")\n",
    "    ax1_acc.grid(alpha=0.3)\n",
    "\n",
    "    # selected count\n",
    "    ax2 = fig.add_subplot(gs[1,1])\n",
    "    for i,m in enumerate(methods):\n",
    "        sub=grp[grp['model_name']==m]\n",
    "        ax2.plot(sub['iteration'],sub['n_selected'],\n",
    "                 color=palette[m],\n",
    "                 linestyle=linestyles[i%len(linestyles)],\n",
    "                 marker=markers[i%len(markers)])\n",
    "    ax2.axhline(grp['n_true_features'].iloc[0],ls='--',color='gray',label='True support')\n",
    "    ax2.set_title(\"Selected Features\"); ax2.set_xlabel(\"Iter\"); ax2.set_ylabel(\"Count\")\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    # legend\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title=\"Model\",\n",
    "               bbox_to_anchor=(0.93,0.75), loc='center left', frameon=False)\n",
    "\n",
    "    # confusion table\n",
    "    ax3 = fig.add_subplot(gs[3,:])\n",
    "    ax3.axis('off')\n",
    "    tbl = ax3.table(cellText=cm_df.values,\n",
    "                    rowLabels=cm_df.index,\n",
    "                    colLabels=cm_df.columns,\n",
    "                    cellLoc='center', loc='center')\n",
    "    tbl.scale(1, 1.2)        # weniger vertikaler Stretch\n",
    "    tbl.auto_set_font_size(True)\n",
    "    tbl.set_fontsize(10)     # etwas kleinere Schrift\n",
    "\n",
    "    plt.show()\n",
    "# -------------------------------\n",
    "# 3) Average F1 Barplot (Matplotlib)\n",
    "# -------------------------------\n",
    "# Nur die gefilterten Methoden für den Durchschnitt verwenden\n",
    "avg_f1 = df[df['model_name'].isin(methods)].groupby('model_name')['best_f1'].mean().sort_values()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [palette[m] for m in avg_f1.index]\n",
    "plt.barh(avg_f1.index, avg_f1.values, color=colors)\n",
    "for i, (val, lab) in enumerate(zip(avg_f1.values, avg_f1.index)):\n",
    "    plt.text(val + 0.01, i, f\"{val:.3f}\", va='center')\n",
    "plt.title(\"Average Predictive F1 Across All Scenarios\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Mean best_f1\", fontsize=12)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 3.5) Average Accuracy Barplot (Matplotlib)\n",
    "# -------------------------------\n",
    "# Only plot accuracy barplot if accuracy_test column exists and has data\n",
    "if 'accuracy_test' in df.columns and not df['accuracy_test'].isna().all():\n",
    "    # Nur die gefilterten Methoden für den Durchschnitt verwenden\n",
    "    avg_acc = df[df['model_name'].isin(methods)].groupby('model_name')['accuracy_test'].mean().sort_values()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = [palette[m] for m in avg_acc.index]\n",
    "    plt.barh(avg_acc.index, avg_acc.values, color=colors)\n",
    "    for i, (val, lab) in enumerate(zip(avg_acc.values, avg_acc.index)):\n",
    "        plt.text(val + 0.01, i, f\"{val:.3f}\", va='center')\n",
    "    plt.title(\"Average Test Accuracy Across All Scenarios\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Mean accuracy_test\", fontsize=12)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy data not available for barplot\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Predictive F1 Heatmap (Mean best_f1)\n",
    "# -------------------------------\n",
    "# Nur die gefilterten Methoden für die Heatmap verwenden\n",
    "heat_df = df[df['model_name'].isin(methods)].pivot_table(\n",
    "    index='scenario', columns='model_name', values='best_f1', aggfunc='mean'\n",
    ")\n",
    "n_scen     = heat_df.shape[0]\n",
    "fig_height = max(4, n_scen * 0.8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, fig_height))\n",
    "vmin, vmax = heat_df.values.min(), heat_df.values.max()\n",
    "sns.heatmap(\n",
    "    heat_df, annot=True, fmt=\".3f\",\n",
    "    cmap=\"RdYlBu_r\", vmin=vmin, vmax=vmax,\n",
    "    cbar_kws={'label': 'Mean Predictive F1'},\n",
    "    linewidths=0.5, linecolor='gray', ax=ax\n",
    ")\n",
    "ax.set_title(\"Mean Predictive F1 by Scenario and Model\", fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel(\"Scenario\", fontsize=12)\n",
    "ax.set_xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "fig.tight_layout(rect=(0, 0, 0.85, 0.88))\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Test Accuracy Heatmap (Mean accuracy_test)\n",
    "# -------------------------------\n",
    "# Only plot accuracy heatmap if accuracy_test column exists and has data\n",
    "if 'accuracy_test' in df.columns and not df['accuracy_test'].isna().all():\n",
    "    # Only the filtered methods for the accuracy heatmap\n",
    "    acc_heat_df = df[df['model_name'].isin(methods)].pivot_table(\n",
    "        index='scenario', columns='model_name', values='accuracy_test', aggfunc='mean'\n",
    "    )\n",
    "    n_scen_acc = acc_heat_df.shape[0]\n",
    "    fig_height_acc = max(4, n_scen_acc * 0.8)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, fig_height_acc))\n",
    "    vmin_acc, vmax_acc = acc_heat_df.values.min(), acc_heat_df.values.max()\n",
    "    sns.heatmap(\n",
    "        acc_heat_df, annot=True, fmt=\".3f\",\n",
    "        cmap=\"RdYlBu_r\", vmin=vmin_acc, vmax=vmax_acc,\n",
    "        cbar_kws={'label': 'Mean Test Accuracy'},\n",
    "        linewidths=0.5, linecolor='gray', ax=ax\n",
    "    )\n",
    "    ax.set_title(\"Mean Test Accuracy by Scenario and Model\", fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel(\"Scenario\", fontsize=12)\n",
    "    ax.set_xlabel(\"Model\", fontsize=12)\n",
    "\n",
    "    fig.tight_layout(rect=(0, 0, 0.85, 0.88))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy data not available for heatmap\")"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: scenario_short'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_f1\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_selected\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_true_features\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m df: df[c] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_numeric(df[c], errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoerce\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 31\u001B[0m short_map \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mscenario\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mscenario_short\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mfirst()\u001B[38;5;241m.\u001B[39mto_dict()\n\u001B[1;32m     32\u001B[0m long_map  \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscenario\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscenario_long\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfirst()\u001B[38;5;241m.\u001B[39mto_dict()\n\u001B[1;32m     33\u001B[0m scenarios \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscenario\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munique())\n",
      "File \u001B[0;32m/opt/anaconda3/envs/thesis_env/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1951\u001B[0m, in \u001B[0;36mDataFrameGroupBy.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1944\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(key) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1945\u001B[0m     \u001B[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001B[39;00m\n\u001B[1;32m   1946\u001B[0m     \u001B[38;5;66;03m# valid syntax, so don't raise\u001B[39;00m\n\u001B[1;32m   1947\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1948\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot subset columns with a tuple with more than one element. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1949\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse a list instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1950\u001B[0m     )\n\u001B[0;32m-> 1951\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/thesis_env/lib/python3.10/site-packages/pandas/core/base.py:244\u001B[0m, in \u001B[0;36mSelectionMixin.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj:\n\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    245\u001B[0m     ndim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj[key]\u001B[38;5;241m.\u001B[39mndim\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gotitem(key, ndim\u001B[38;5;241m=\u001B[39mndim)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Column not found: scenario_short'"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
