# Roadmap
- different approaches of NIMO
- snythetic data more (how to create synthetic data)
- learn about theory, about the steps
- difference in NIMO and MyVersion
- make some experiments with different implementations (include original nimo)

# Notes:
- What i did for changes in nimo official:
   - deleted cuda stuff, gpu stuff 

Stuff to learn:
- forwardâ€pass, the IRLS + adaptiveâ€ridge updates, the nonlinearity, the zeroâ€mean correction or the groupâ€Lasso proximal step 

# NIMO Aktuell

- In deiner aktuellen NIMO-Implementierung fÃ¼hrst du im IRLS-Schritt zwar eine Ridge-Regression durch
   - das ist aber eine klassische, nicht-adaptive Ridge mit festem Î» fÃ¼r alle Koeffizienten.
   - ich nutze eine einheitliche Ridge (ğœ†*ğ¼), also keine adaptive Gewichtung. 
   - LÃ¶sung: Um das einzubauen, mÃ¼sstest du nach jedem IRLS-Schritt die w_j (zB 1/beta_j^gamma) neu berechnen und in Matrix A als diag (ğœ†*w) statt (ğœ†*ğ¼ einsetzen)


# Questions
- do i need to have GPU or something to run NIMO?
- i have always 0.4 f1 score. waht do you think is it because of dataset, all emthdos aournd 0.4
- How is nimo differentiating between  logistic and continuous target variables?


# TODO Code
- Mini-Batches
- automate Hyparparameter selection
- Early-Stopping & Convergence-Criterion
  - find out if it makes sense to have an early stopping. make debugs and see the trend of loss, if at some point it converges or even gets worse
- Tanh/Sinus-Activations & Dropout:
  - Du hast schon Tanh() und SinAct(). Du kÃ¶nntest testen, ob es hilft, das Sinus-Layer vor oder nach der zweiten FC zu setzen, oder mit verschiedenen Skalen (z.B. sin(Î± x)) zu spielen. 
  - Ebenfalls kÃ¶nntest du experimentieren, ob ein zusÃ¤tzliches Dropout in der ersten Schicht (statt nur einmal) Ãœberanpassung weiter vermeidet.
- Adaptive-Ridge (Adaptive-Lasso) weiter verfeinern:
  - Im Paper empfehlen sie, das Î³-Update (w = 1/|Î²|^Î³) auch innerhalb des IRLS-Loops etwas anzupassen (z.B. Î³ â† Î³Â·c oder mit kleinem Learning-Rate), um die Sparsity schÃ¤rfer zu kontrollieren.
- Logging von Training und Î²-Werten:
  - Sammle nach jeder Iteration den Wert von âˆ¥Î²âˆ¥â‚€ (Anzahl non-zero Î²) und âˆ¥Î²âˆ¥â‚‚, und plotte den Verlauf, um zu sehen, wie schnell deine adaptive-Lasso-Sparsity einsetzt.